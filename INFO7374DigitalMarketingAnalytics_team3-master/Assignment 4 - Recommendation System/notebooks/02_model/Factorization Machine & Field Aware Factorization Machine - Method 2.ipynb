{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing the dependencies </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlearn as xl\n",
    "train = pd.read_csv('SnackDataset.csv', encoding= 'unicode_escape')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating dataframe for our model training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Category','Price','Month','Purchase Made']\n",
    "train_sub = train[cols]\n",
    "dict_ls = {'Y':1, 'N':0}\n",
    "train_sub['Purchase Made'].replace(dict_ls, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Splitting our model for training & testing </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(train_sub, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics =['Price','Purchase Made']\n",
    "categories = ['Category', 'Purchase Made', 'Month']\n",
    "features = ['Price', 'Category', 'Month' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Converting our dataframe to libffm format for our model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ffm(df,type,numerics,categories,features):\n",
    "    currentcode = len(numerics)\n",
    "    catdict = {}\n",
    "    catcodes = {}\n",
    "    \n",
    "    # Flagging categorical and numerical fields\n",
    "    for x in numerics:\n",
    "         catdict[x] = 0\n",
    "    for x in categories:\n",
    "         catdict[x] = 1\n",
    "    \n",
    "    nrows = df.shape[0]\n",
    "    ncolumns = len(features)\n",
    "    with open(str(type) + \"_ffm.txt\", \"w\") as text_file:\n",
    "    \n",
    "    # Looping over rows to convert each row to libffm format\n",
    "        for n, r in enumerate(range(nrows)):\n",
    "            datastring = \"\"\n",
    "            datarow = df.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['Purchase Made'])) # Set Target Variable here\n",
    "             \n",
    "            # For numerical fields, we are creating a dummy field here\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "            \n",
    "            # For a new field appearing in a training example\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode #encoding the feature\n",
    "             \n",
    "            # For already encoded fields\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                         currentcode +=1\n",
    "                         catcodes[x][datarow[x]] = currentcode #encoding the feature\n",
    "                     \n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "\n",
    "            datastring += '\\n'\n",
    "            text_file.write(datastring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Converting train dataset to libffm format </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_ffm(X_train, 'train', numerics, categories, features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Prediction using xlearn </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm_model = xl.create_ffm()\n",
    "\n",
    "ffm_model.setTrain(\"train_ffm.txt\")\n",
    "\n",
    "param = {'task':'binary', \n",
    "         'lr':0.2,\n",
    "         'lambda':0.002, \n",
    "         'metric':'acc'}\n",
    "\n",
    "# Start to train\n",
    "# The trained model will be stored in model.out\n",
    "ffm_model.fit(param, './model.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Converting test dataset to libffm format </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_ffm(X_train, 'test', numerics, categories, features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction task\n",
    "ffm_model.setTest(\"test_ffm.txt\") # Test data\n",
    "ffm_model.setSign() # Convert output to 0-1\n",
    "\n",
    "# Start to predict\n",
    "# The output result will be stored in output.txt\n",
    "ffm_model.predict(\"./model.out\", \"./output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Evaluation - AUC Score </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output.txt\") as f:\n",
    "    predictions = f.readlines()\n",
    "\n",
    "with open(\"test_ffm.txt\") as f:\n",
    "    truths = f.readlines()\n",
    "\n",
    "truths = np.array([float(truth.split(' ')[0]) for truth in truths])\n",
    "predictions = np.array([float(prediction.strip('')) for prediction in predictions])\n",
    "\n",
    "auc_score = roc_auc_score(truths, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
